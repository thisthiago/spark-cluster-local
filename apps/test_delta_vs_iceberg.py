"""
ComparaÃ§Ã£o prÃ¡tica entre Delta Lake e Apache Iceberg
Demonstra as mesmas operaÃ§Ãµes em ambos os formatos
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp
import time

print("=" * 70)
print("Delta Lake vs Apache Iceberg - ComparaÃ§Ã£o PrÃ¡tica")
print("=" * 70)

# Criar SparkSession com ambos configurados
spark = SparkSession.builder \
    .appName("Delta-vs-Iceberg-Comparison") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "hadoop") \
    .config("spark.sql.catalog.iceberg.warehouse", "/data/iceberg-warehouse") \
    .getOrCreate()

# Dados de exemplo
data = [
    (1, "Produto A", 100, 10.50, "2025-10-20"),
    (2, "Produto B", 200, 20.00, "2025-10-21"),
    (3, "Produto C", 150, 15.75, "2025-10-22"),
    (4, "Produto D", 300, 30.00, "2025-10-23"),
    (5, "Produto E", 250, 25.50, "2025-10-24"),
]

columns = ["id", "nome", "quantidade", "preco", "data"]
df = spark.createDataFrame(data, columns)

print("\nğŸ“Š Dados de teste:")
df.show()

# ========== DELTA LAKE ==========
print("\n" + "=" * 70)
print("ğŸ”· DELTA LAKE")
print("=" * 70)

# 1. Criar tabela Delta
print("\n1. Criando tabela Delta...")
start = time.time()
delta_path = "/data/delta/produtos"
df.write.format("delta").mode("overwrite").save(delta_path)
delta_create_time = time.time() - start
print(f"âœ“ Tabela Delta criada em {delta_create_time:.3f}s")

# 2. Ler Delta
print("\n2. Lendo tabela Delta...")
start = time.time()
df_delta = spark.read.format("delta").load(delta_path)
delta_read_time = time.time() - start
print(f"âœ“ Leitura concluÃ­da em {delta_read_time:.3f}s")
print(f"   Registros: {df_delta.count()}")

# 3. Update Delta
print("\n3. Update em Delta...")
start = time.time()
from delta.tables import DeltaTable
deltaTable = DeltaTable.forPath(spark, delta_path)
deltaTable.update(
    condition = "id = 1",
    set = {"preco": "12.00"}
)
delta_update_time = time.time() - start
print(f"âœ“ Update concluÃ­do em {delta_update_time:.3f}s")

# 4. Delete Delta
print("\n4. Delete em Delta...")
start = time.time()
deltaTable.delete("id = 5")
delta_delete_time = time.time() - start
print(f"âœ“ Delete concluÃ­do em {delta_delete_time:.3f}s")

# 5. Merge Delta
print("\n5. Merge (Upsert) em Delta...")
start = time.time()
updates = spark.createDataFrame([
    (6, "Produto F", 400, 40.00, "2025-10-25"),
    (1, "Produto A+", 100, 12.00, "2025-10-20")
], columns)

deltaTable.alias("target").merge(
    updates.alias("source"),
    "target.id = source.id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
delta_merge_time = time.time() - start
print(f"âœ“ Merge concluÃ­do em {delta_merge_time:.3f}s")

# 6. Time Travel Delta
print("\n6. Time Travel em Delta...")
history = deltaTable.history()
print(f"   VersÃµes disponÃ­veis: {history.count()}")
df_v0 = spark.read.format("delta").option("versionAsOf", 0).load(delta_path)
print(f"   Registros na versÃ£o 0: {df_v0.count()}")

# 7. Schema Evolution Delta
print("\n7. Schema Evolution em Delta...")
start = time.time()
df_new_schema = spark.createDataFrame([
    (7, "Produto G", 500, 50.00, "2025-10-26", "Nova Categoria")
], columns + ["categoria"])
df_new_schema.write.format("delta").mode("append") \
    .option("mergeSchema", "true").save(delta_path)
delta_schema_time = time.time() - start
print(f"âœ“ Schema Evolution em {delta_schema_time:.3f}s")

# ========== APACHE ICEBERG ==========
print("\n" + "=" * 70)
print("ğŸ”¶ APACHE ICEBERG")
print("=" * 70)

# 1. Criar tabela Iceberg
print("\n1. Criando tabela Iceberg...")
start = time.time()
spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg.comparacao")
df.write.format("iceberg") \
    .mode("overwrite") \
    .saveAsTable("iceberg.comparacao.produtos")
iceberg_create_time = time.time() - start
print(f"âœ“ Tabela Iceberg criada em {iceberg_create_time:.3f}s")

# 2. Ler Iceberg
print("\n2. Lendo tabela Iceberg...")
start = time.time()
df_iceberg = spark.table("iceberg.comparacao.produtos")
iceberg_read_time = time.time() - start
print(f"âœ“ Leitura concluÃ­da em {iceberg_read_time:.3f}s")
print(f"   Registros: {df_iceberg.count()}")

# 3. Update Iceberg
print("\n3. Update em Iceberg...")
start = time.time()
spark.sql("""
    UPDATE iceberg.comparacao.produtos
    SET preco = 12.00
    WHERE id = 1
""")
iceberg_update_time = time.time() - start
print(f"âœ“ Update concluÃ­do em {iceberg_update_time:.3f}s")

# 4. Delete Iceberg
print("\n4. Delete em Iceberg...")
start = time.time()
spark.sql("DELETE FROM iceberg.comparacao.produtos WHERE id = 5")
iceberg_delete_time = time.time() - start
print(f"âœ“ Delete concluÃ­do em {iceberg_delete_time:.3f}s")

# 5. Merge Iceberg
print("\n5. Merge (Upsert) em Iceberg...")
start = time.time()
updates.createOrReplaceTempView("updates")
spark.sql("""
    MERGE INTO iceberg.comparacao.produtos t
    USING updates s
    ON t.id = s.id
    WHEN MATCHED THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
""")
iceberg_merge_time = time.time() - start
print(f"âœ“ Merge concluÃ­do em {iceberg_merge_time:.3f}s")

# 6. Time Travel Iceberg
print("\n6. Time Travel em Iceberg...")
snapshots = spark.sql("SELECT * FROM iceberg.comparacao.produtos.snapshots")
print(f"   Snapshots disponÃ­veis: {snapshots.count()}")
first_snapshot = snapshots.select("snapshot_id").first()[0]
df_snap = spark.read.option("snapshot-id", first_snapshot) \
    .table("iceberg.comparacao.produtos")
print(f"   Registros no primeiro snapshot: {df_snap.count()}")

# 7. Schema Evolution Iceberg
print("\n7. Schema Evolution em Iceberg...")
start = time.time()
spark.sql("""
    ALTER TABLE iceberg.comparacao.produtos 
    ADD COLUMN categoria STRING
""")
spark.sql("""
    INSERT INTO iceberg.comparacao.produtos
    VALUES (7, 'Produto G', 500, 50.00, '2025-10-26', 'Nova Categoria')
""")
iceberg_schema_time = time.time() - start
print(f"âœ“ Schema Evolution em {iceberg_schema_time:.3f}s")

# ========== COMPARAÃ‡ÃƒO DE PERFORMANCE ==========
print("\n" + "=" * 70)
print("ğŸ“Š COMPARAÃ‡ÃƒO DE PERFORMANCE")
print("=" * 70)

print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OperaÃ§Ã£o             â”‚ Delta Lake   â”‚ Iceberg      â”‚ Vencedor     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CREATE TABLE         â”‚ {delta_create_time:>8.3f}s    â”‚ {iceberg_create_time:>8.3f}s    â”‚ {'Delta' if delta_create_time < iceberg_create_time else 'Iceberg':>12s} â”‚
â”‚ READ                 â”‚ {delta_read_time:>8.3f}s    â”‚ {iceberg_read_time:>8.3f}s    â”‚ {'Delta' if delta_read_time < iceberg_read_time else 'Iceberg':>12s} â”‚
â”‚ UPDATE               â”‚ {delta_update_time:>8.3f}s    â”‚ {iceberg_update_time:>8.3f}s    â”‚ {'Delta' if delta_update_time < iceberg_update_time else 'Iceberg':>12s} â”‚
â”‚ DELETE               â”‚ {delta_delete_time:>8.3f}s    â”‚ {iceberg_delete_time:>8.3f}s    â”‚ {'Delta' if delta_delete_time < iceberg_delete_time else 'Iceberg':>12s} â”‚
â”‚ MERGE (Upsert)       â”‚ {delta_merge_time:>8.3f}s    â”‚ {iceberg_merge_time:>8.3f}s    â”‚ {'Delta' if delta_merge_time < iceberg_merge_time else 'Iceberg':>12s} â”‚
â”‚ SCHEMA EVOLUTION     â”‚ {delta_schema_time:>8.3f}s    â”‚ {iceberg_schema_time:>8.3f}s    â”‚ {'Delta' if delta_schema_time < iceberg_schema_time else 'Iceberg':>12s} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# ========== FEATURES EXCLUSIVAS ==========
print("\n" + "=" * 70)
print("âš¡ FEATURES EXCLUSIVAS")
print("=" * 70)

print("\nğŸ”· Delta Lake 4.0:")
print("  â€¢ Liquid Clustering (otimizaÃ§Ã£o automÃ¡tica)")
print("  â€¢ Delta Connect (integraÃ§Ã£o com Spark Connect)")
print("  â€¢ Melhor integraÃ§Ã£o com Databricks")
print("  â€¢ Z-Ordering nativo")
print("  â€¢ Change Data Feed (CDC)")
print("  â€¢ Deletion Vectors (performance em deletes)")

print("\nğŸ”¶ Apache Iceberg:")
print("  â€¢ Partition Evolution (mudar particionamento sem reescrever)")
print("  â€¢ Hidden Partitioning (usuÃ¡rio nÃ£o vÃª partiÃ§Ãµes)")
print("  â€¢ Multi-engine support (Spark, Flink, Trino, Presto)")
print("  â€¢ Table Format aberto (Apache Foundation)")
print("  â€¢ Metadata Tables robustas")
print("  â€¢ Sort Order customizÃ¡vel")

# ========== QUANDO USAR CADA UM ==========
print("\n" + "=" * 70)
print("ğŸ¯ QUANDO USAR?")
print("=" * 70)

print("\nâœ… Use DELTA LAKE quando:")
print("  â€¢ JÃ¡ usa Databricks")
print("  â€¢ Precisa de Liquid Clustering")
print("  â€¢ Quer integraÃ§Ã£o nativa com Delta Live Tables")
print("  â€¢ Precisa de Change Data Feed")
print("  â€¢ Performance Ã© prioridade mÃ¡xima")
print("  â€¢ Ecosystem Databricks Ã© importante")

print("\nâœ… Use APACHE ICEBERG quando:")
print("  â€¢ Precisa de multi-engine support (Trino, Flink, etc)")
print("  â€¢ Quer formato open source neutro")
print("  â€¢ Precisa de Partition Evolution")
print("  â€¢ Trabalha em ambiente multi-cloud")
print("  â€¢ Prefere padrÃµes Apache")
print("  â€¢ Precisa de flexibilidade mÃ¡xima")

# ========== ANÃLISE DE METADADOS ==========
print("\n" + "=" * 70)
print("ğŸ“ ANÃLISE DE METADADOS")
print("=" * 70)

print("\nğŸ”· Delta Lake - Tamanho de arquivos:")
import os
delta_size = 0
for root, dirs, files in os.walk("/data/delta/produtos"):
    for file in files:
        delta_size += os.path.getsize(os.path.join(root, file))
print(f"   Total: {delta_size / 1024:.2f} KB")

print("\nğŸ”¶ Iceberg - Metadata Tables:")
print("\n   Files:")
spark.sql("SELECT COUNT(*) as total_files FROM iceberg.comparacao.produtos.files").show()
print("\n   Snapshots:")
spark.sql("SELECT COUNT(*) as total_snapshots FROM iceberg.comparacao.produtos.snapshots").show()
print("\n   Manifests:")
spark.sql("SELECT COUNT(*) as total_manifests FROM iceberg.comparacao.produtos.manifests").show()

# ========== CONSULTAS FINAIS ==========
print("\n" + "=" * 70)
print("ğŸ“‹ DADOS FINAIS (Delta Lake):")
print("=" * 70)
df_delta_final = spark.read.format("delta").load(delta_path)
df_delta_final.orderBy("id").show()

print("\n" + "=" * 70)
print("ğŸ“‹ DADOS FINAIS (Iceberg):")
print("=" * 70)
df_iceberg_final = spark.table("iceberg.comparacao.produtos")
df_iceberg_final.orderBy("id").show()

# ========== VERIFICAÃ‡ÃƒO DE CONSISTÃŠNCIA ==========
print("\n" + "=" * 70)
print("ğŸ” VERIFICAÃ‡ÃƒO DE CONSISTÃŠNCIA")
print("=" * 70)

delta_count = df_delta_final.count()
iceberg_count = df_iceberg_final.count()

print(f"\n   Delta Lake registros: {delta_count}")
print(f"   Iceberg registros: {iceberg_count}")

if delta_count == iceberg_count:
    print("\n   âœ… Ambos tÃªm a mesma quantidade de registros!")
else:
    print("\n   âš ï¸  DiferenÃ§a na quantidade de registros")

# ========== RECOMENDAÃ‡Ã•ES ==========
print("\n" + "=" * 70)
print("ğŸ’¡ RECOMENDAÃ‡Ã•ES FINAIS")
print("=" * 70)
print("""
Para um cluster Spark moderno, considere:

1. ğŸ¯ CENÃRIO ÃšNICO (Uma engine):
   â†’ Use Delta Lake se jÃ¡ estÃ¡ no ecossistema Databricks
   â†’ Use Iceberg se prefere Apache open source

2. ğŸŒ CENÃRIO MULTI-ENGINE (Spark + Trino + Flink):
   â†’ Use Iceberg para mÃ¡xima compatibilidade

3. ğŸš€ PERFORMANCE CRÃTICA:
   â†’ Delta Lake com Liquid Clustering (Spark 4.0)
   â†’ Iceberg com Sort Order bem configurado

4. ğŸ”„ EVOLUTIVO (Schema/Partition changes frequentes):
   â†’ Iceberg para Partition Evolution
   â†’ Delta Lake para Schema Evolution simples

5. ğŸ’° CUSTO (Storage):
   â†’ Ambos sÃ£o eficientes com compaction
   â†’ Iceberg pode ser ligeiramente mais eficiente em metadata

ğŸ† VENCEDOR GERAL: Depende do seu caso de uso!
   â€¢ Delta Lake: Melhor para ecosistema Databricks
   â€¢ Iceberg: Melhor para multi-engine e neutralidade
""")

print("\n" + "=" * 70)
print("âœ… ComparaÃ§Ã£o concluÃ­da!")
print("=" * 70)

spark.stop()